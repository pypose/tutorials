{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# LieTensor Tutorial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport pypose as pp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Intialization\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = pp.so3(torch.randn(2,3))\nx = pp.identity_SE3(2,1)\ny = pp.randn_se3(2,2)\nprint('a:', a, '\\nx.shape:', x.shape, '\\nx.gshape:', x.lshape)\nprint(x.lview(2))\nprint(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### All arguments in PyTorch are supported\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = pp.randn_SO3(3, device=\"cuda:0\", dtype=torch.double, requires_grad=True)\nb = pp.identity_like(a, device=\"cpu\")\na, b\nt = a.float()\na, t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Slicing and Shaping\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A = pp.randn_SO3(2,2)\nB = pp.randn_SO3(2,1)\nC = torch.cat([A,B], dim=1)         # Tensor cat\nC[0,1] = pp.randn_SO3(1)            # Slicing set\nD = C[1,:].Log()                    # Slicing get\nE, F = torch.split(C, [1,2], dim=1) # Tensor split\nprint('A:', A.lshape)\nprint('B:', B.lshape)\nprint('C:', C.lshape)\nprint('D:', D.lshape)\nprint('E:', E.lshape)\nprint('F:', F.lshape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Basic Operations\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(x * y.Exp()).Inv().Log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Adjoint Transforms\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = pp.randn_Sim3(6, dtype=torch.double)\na = pp.randn_sim3(6, dtype=torch.double)\nb = X.AdjT(a)\nprint((X * b.Exp() - a.Exp() * X).abs().mean() < 1e-7)\n\nX = pp.randn_SE3(8)\na = pp.randn_se3(8)\nb = X.Adj(a)\nprint((b.Exp() * X - X * a.Exp()).abs().mean() < 1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Grdients\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = pp.randn_so3(3, sigma=0.1, requires_grad=True, device=\"cuda\")\nassert x.is_leaf\nloss = (x.Exp().Log()**2).sin().sum() # Just test, No physical meaning\nloss.backward()\ny = x.detach()\nloss, x.grad, x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test a Module\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nclass TestNet(nn.Module):\n    def __init__(self, n):\n        super().__init__()\n        self.weight = pp.Parameter(pp.randn_so3(n))\n\n    def forward(self, x):\n        return self.weight.Exp() * x\n\n\nn,epoch = 4, 5\nnet = TestNet(n).cuda()\n\noptimizer = torch.optim.SGD(net.parameters(), lr = 0.2, momentum=0.9)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,4], gamma=0.5)\n\nprint(\"Before Optimization:\\n\", net.weight)\nfor i in range(epoch):\n    optimizer.zero_grad()\n    inputs = pp.randn_SO3(n).cuda()\n    outputs = net(inputs)\n    loss = outputs.abs().sum()\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n    print(loss)\n\nprint(\"Parameter:\", count_parameters(net))\nprint(\"After Optimization:\\n\", net.weight)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
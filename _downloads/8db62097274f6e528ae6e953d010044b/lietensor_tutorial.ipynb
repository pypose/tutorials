{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# LieTensor Tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``LieTensor`` is the cornerstone of PyPose project. ``LieTensor`` is derived from\n``torch.tensor``. it represents Lie Algebra or Lie Group. It support all \nthe ``torch.tensor`` features and also specific features for Lie Theory.\n\n\nWe will see eventually in this tutorial that, with ``LieTensor``,\none could easily implement operations often used in robotics applications.\n\nIn PyPose, we would want to utilize the powerful network training API the comes with PyTorch.\nSo, we will go a step further to see how we can use ``LieTensor``` in training a simple network.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport pypose as pp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Intialization\nThe first thing we need to know is how to initialize a LieTensor.\nUse ``pypose.LieTensor`` or alias like ``pypose.so3``, specify the ``data`` and ``ltpye``.\nSee list of ``ltype`` \n[here](https://pypose.org/docs/main/generated/pypose.LieTensor/#pypose.LieTensor).\n\nNote that the last dimension\nof ``data`` has to align with the ``LieTensor.ltype.dimension``\nbecause LieTensor has different length with respect to different ``ltype``.\nHere we have a ``(2,3)`` shaped tensor, because ``so3_type`` \nrequires a dimension of 3 for each element.\n\nIt is recommanded to use alias to initialize LieTensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = torch.randn(2, 3, requires_grad=True, device='cuda:0')\na = pp.LieTensor(data, ltype=pp.so3_type)\nprint('a:', a)\nb = pp.so3(data)\nprint('b:', b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like ``PyTorch``, you can initialize an identity ``LieTensor`` or a random ``LieTensor``.\nUse the function related to each ``ltype``. For example, here we used ``pypose.identity_SE3``\nand ``pypose.randn_se3``. The usage is similar with `torch.randn`, except the shape we input\nis ``lshape``.\nThe only difference between ``LieTensor.lshape`` and ``tensor.shape`` is the last dimension is hidden, since\n``lshape`` takes the last dimension as a single ``ltype`` item.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You might notice the case difference here. \nIn PyPose, uppercase refers to Lie Group, and lowercase refers to Lie Algebra.\nOften, we use Lie Group to represent transformation. \nBut, when used in optimization or backpropagated, Lie Algebra is needed.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``LieTensor.lview`` here is used to change the shape of a ``LieTensor``,\nsimilar to ``torch.view``.\nThe difference is that ``LieTensor.lview`` does not modify the last dimension.\nIt is intuitive since we need each element in ``x`` stays a ``SE3`` ltype.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = pp.identity_SE3(2,1)\ny = pp.randn_se3(2,2)\nprint('x.shape:', x.shape, '\\nx.gshape:', x.lshape)\nprint(x.lview(2))\nprint(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. All arguments in PyTorch are supported\n``LieTensor`` is derived from ``torch.tensor``, so it inherit all the \nattributes of a ``tensor``.\nYou could specify ``device``, ``dtype``, and ``requires_grad`` during the initialization,\njust like PyTorch. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = pp.randn_SO3(3, device=\"cuda:0\", dtype=torch.double, requires_grad=True)\nb = pp.identity_like(a, device=\"cpu\")\na, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And also, easy data type transform.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t = a.float()\na, t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slicing and Shaping\n``LieTensor`` concatination is also the same as ``Pytorch``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A = pp.randn_SO3(2,2)\nB = pp.randn_SO3(2,1)\nC = torch.cat([A,B], dim=1)         # Tensor cat\nC[0,1] = pp.randn_SO3(1)            # Slicing set\nD = C[1,:].Log()                    # Slicing get\nE, F = torch.split(C, [1,2], dim=1) # Tensor split\nprint('A:', A.lshape)\nprint('B:', B.lshape)\nprint('C:', C.lshape)\nprint('D:', D.lshape)\nprint('E:', E.lshape)\nprint('F:', F.lshape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exponential, Logarithm and Inversion Function\n``LieTensor.Exp`` is the Exponential function defined in Lie Theory,\nwhich transform a input Lie Algebra to Lie Group. \n``LieTensor.Log`` is the Logarithm function, whcih transform Lie Group back to Lie Algebra.\nSee the doc of\n[LieTensor.Exp](https://pypose.org/docs/main/generated/pypose.Exp/) and \n[LieTensor.Log](https://pypose.org/docs/main/generated/pypose.Log/)\nfor the math.\n\n``LieTensor.Inv`` gives us the inversion of a ``LieTensor``.\nAssume you have a ``LieTensor`` of ``pypose.so3_type``\nrepresenting a rotation $R$, the `Inv` will give you $R^{-1}$.\nSee [LieTensor.Inv](https://pypose.org/docs/main/generated/pypose.Inv/).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(x * y.Exp()).Inv().Log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Adjoint Transforms\nWe also have adjoint operations. Assume ``X`` is a Lie Group, \nand ``a`` is a small left increment in Lie Algebra. \nAdjoint operation will input ``a`` and output a right increment ``b`` that gives ther same transformation.\nSee [pypose.Adj](https://pypose.org/docs/main/generated/pypose.Adj/) for more details.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = pp.randn_Sim3(6, dtype=torch.double)\na = pp.randn_sim3(6, dtype=torch.double)\nb = X.AdjT(a)\nprint((X * b.Exp() - a.Exp() * X).abs().mean() < 1e-7)\n\nX = pp.randn_SE3(8)\na = pp.randn_se3(8)\nb = X.Adj(a)\nprint((b.Exp() * X - X * a.Exp()).abs().mean() < 1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Grdients\nAs mentioned at the beginning, we would want to utilize the powerful\nnetwork training API the comes with PyTorch.\nWe might want to start by calculating gradients,\nwhich is a core step of any network training.\nFirst, we need to initialize the ``LieTensor`` of which we want to get gradients.\nRemember to set ``requires_grad=True``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = pp.randn_so3(3, sigma=0.1, requires_grad=True, device=\"cuda\")\nassert x.is_leaf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And, just like in PyTorch, we will define a ``loss``, and call ``loss.backward``.\nThat's it. Exactly the same with PyTorch.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = (x.Exp().Log()**2).sin().sum() # Just test, No physical meaning\nloss.backward()\ny = x.detach()\nloss, x.grad, x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test a Module\nNow that we know all the basic operations, we might start ahead to build our first network.\nFirst of all, we define our ``TestNet`` as follows. Still, it doesn't have any physical meaning.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nclass TestNet(nn.Module):\n    def __init__(self, n):\n        super().__init__()\n        self.weight = pp.Parameter(pp.randn_so3(n))\n\n    def forward(self, x):\n        return self.weight.Exp() * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Still, like PyTorch, we instantiate our network, optimizer, and scheduler.\nScheduler here is to control the learning rate, see [lr_scheduler.MultiStepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR)\nfor more detail.\n\nThen, inside the loop, we run our training. If you are not familiar with the training process,\nwe would recommand you reading one of the PyTorch tutorial, like \n[this](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n,epoch = 4, 5\nnet = TestNet(n).cuda()\n\noptimizer = torch.optim.SGD(net.parameters(), lr = 0.2, momentum=0.9)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,4], gamma=0.5)\n\nprint(\"Before Optimization:\\n\", net.weight)\nfor i in range(epoch):\n    optimizer.zero_grad()\n    inputs = pp.randn_SO3(n).cuda()\n    outputs = net(inputs)\n    loss = outputs.abs().sum()\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n    print(loss)\n\nprint(\"Parameter:\", count_parameters(net))\nprint(\"After Optimization:\\n\", net.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then we are finished with our ``LieTensor`` tutorial.\nHopefully you are more familiar with it by now.\n\nNow you may be free to explore other tutorials. \nSee How PyPose can be utilized in real robotics applications.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}